{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 导库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, argparse, time, datetime, sys, shutil, stat, torch\n",
    "import torchvision.models as models\n",
    "import torchvision.utils as vutils\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "from util.MF_dataset import MF_dataset\n",
    "from util.util import compute_results, visualize\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from scipy.io import savemat\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from util.augmentation import RandomFlip, RandomCrop, RandomCropOut, RandomBrightness, RandomNoise\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision.models import ResNet152_Weights, ResNet50_Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型搭建"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 注意力机制"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionFusion(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.attetion = nn.Sequential(\n",
    "            nn.Conv2d(channels * 2, channels // 2, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(channels // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(channels // 2, 2, kernel_size=3, padding=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.rgb_confidence = nn.Parameter(torch.tensor(1.0))\n",
    "        self.thermal_confidence = nn.Parameter(torch.tensor(1.0))\n",
    "\n",
    "        self.register_buffer('rgb_prior', torch.ones(1))\n",
    "        self.register_buffer('thermal_prior', torch.ones(1))\n",
    "        self.momentum = 0.9\n",
    "\n",
    "    def update(self, rgb_purity, thermal_purity):\n",
    "        self.rgb_prior = self.momentum * self.rgb_prior + (1-self.momentum)* rgb_purity\n",
    "        self.thermal_prior = self.momentum * self.thermal_prior + (1-self.momentum) * thermal_purity\n",
    "        self.rgb_prior.data.clamp_(0.1, 1.0)\n",
    "        self.thermal_prior.data.clamp_(0.1, 1.0)\n",
    "\n",
    "    def forward(self, rgb_feat, thermal_feat):\n",
    "        combined = torch.cat([rgb_feat, thermal_feat], dim=1)\n",
    "        attention_weights = self.attention(combined)  # [B,2,H,W]\n",
    "\n",
    "        # 将置信度参数转换为与attention_weights相同的维度\n",
    "        rgb_conf = torch.sigmoid(self.rgb_confidence).view(1,1,1,1)  # [1,1,1,1]\n",
    "        thermal_conf = torch.sigmoid(self.thermal_confidence).view(1,1,1,1)\n",
    "\n",
    "        # 动态调整权重（关键修改点）\n",
    "        adjusted_weights = torch.cat([\n",
    "            attention_weights[:,0:1] * rgb_conf * self.rgb_prior,\n",
    "            attention_weights[:,1:2] * thermal_conf * self.thermal_prior\n",
    "        ], dim=1)\n",
    "\n",
    "        # 重新归一化\n",
    "        norm_weights = adjusted_weights / (adjusted_weights.sum(dim=1, keepdim=True) + 1e-8)\n",
    "\n",
    "        return rgb_feat * norm_weights[:,0:1] + thermal_feat * norm_weights[:,1:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 网络定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RTFNet(nn.Module):\n",
    "\n",
    "    def __init__(self, n_class):\n",
    "        super(RTFNet, self).__init__()  # 初始化\n",
    "\n",
    "        self.num_resnet_layers = 152\n",
    "\n",
    "        if self.num_resnet_layers == 50:\n",
    "            resnet_raw_model1 = models.resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "            resnet_raw_model2 = models.resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "            self.inplanes = 2048\n",
    "        elif self.num_resnet_layers == 152:\n",
    "            resnet_raw_model1 = models.resnet152(weights=ResNet152_Weights.DEFAULT)\n",
    "            resnet_raw_model2 = models.resnet152(weights=ResNet152_Weights.DEFAULT)\n",
    "            self.inplanes = 2048\n",
    "\n",
    "        ########  Thermal ENCODER  ########\n",
    "\n",
    "        self.encoder_thermal_conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.encoder_thermal_conv1.weight.data = torch.unsqueeze(torch.mean(resnet_raw_model1.conv1.weight.data, dim=1),\n",
    "                                                                 dim=1)\n",
    "        self.encoder_thermal_bn1 = resnet_raw_model1.bn1\n",
    "        self.encoder_thermal_relu = resnet_raw_model1.relu\n",
    "        self.encoder_thermal_maxpool = resnet_raw_model1.maxpool\n",
    "        self.encoder_thermal_layer1 = resnet_raw_model1.layer1\n",
    "        self.encoder_thermal_layer2 = resnet_raw_model1.layer2\n",
    "        self.encoder_thermal_layer3 = resnet_raw_model1.layer3\n",
    "        self.encoder_thermal_layer4 = resnet_raw_model1.layer4\n",
    "\n",
    "        ########  RGB ENCODER  ########\n",
    "\n",
    "        self.encoder_rgb_conv1 = resnet_raw_model2.conv1\n",
    "        self.encoder_rgb_bn1 = resnet_raw_model2.bn1\n",
    "        self.encoder_rgb_relu = resnet_raw_model2.relu\n",
    "        self.encoder_rgb_maxpool = resnet_raw_model2.maxpool\n",
    "        self.encoder_rgb_layer1 = resnet_raw_model2.layer1\n",
    "        self.encoder_rgb_layer2 = resnet_raw_model2.layer2\n",
    "        self.encoder_rgb_layer3 = resnet_raw_model2.layer3\n",
    "        self.encoder_rgb_layer4 = resnet_raw_model2.layer4\n",
    "\n",
    "        ########  Fusion   ########\n",
    "        self.fusion1 = AttentionFusion(64)\n",
    "        self.fusion2 = AttentionFusion(256)\n",
    "        self.fusion3 = AttentionFusion(512)\n",
    "        self.fusion4 = AttentionFusion(1024)\n",
    "        self.fusion5 = AttentionFusion(2048)\n",
    "        self.fusion1st = AttentionFusion(2)\n",
    "\n",
    "        self.register_buffer('last_rgb_pred', None)\n",
    "        self.register_buffer('last_thermal_feat', None)\n",
    "        self.register_buffer('last_purity', torch.zeros(2))\n",
    "\n",
    "        ########  DECODER  ########\n",
    "\n",
    "        self.deconv1 = self._make_transpose_layer(TransBottleneck, self.inplanes // 2, 2,\n",
    "                                                  stride=2)  # using // for python 3.6\n",
    "        self.deconv2 = self._make_transpose_layer(TransBottleneck, self.inplanes // 2, 2,\n",
    "                                                  stride=2)  # using // for python 3.6\n",
    "        self.deconv3 = self._make_transpose_layer(TransBottleneck, self.inplanes // 2, 2,\n",
    "                                                  stride=2)  # using // for python 3.6\n",
    "        self.deconv4 = self._make_transpose_layer(TransBottleneck, self.inplanes // 2, 2,\n",
    "                                                  stride=2)  # using // for python 3.6\n",
    "        self.deconv5 = self._make_transpose_layer(TransBottleneck, n_class, 2, stride=2)\n",
    "\n",
    "        self.side_conv1 = nn.Conv2d(1024, n_class, kernel_size=1)\n",
    "        self.side_conv2 = nn.Conv2d(512, n_class, kernel_size=1)\n",
    "        self.side_conv3 = nn.Conv2d(256, n_class, kernel_size=1)\n",
    "        self.side_conv4 = nn.Conv2d(128, n_class, kernel_size=1)\n",
    "\n",
    "        self.detail_enhance = nn.Sequential(\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, n_class, kernel_size=1)\n",
    "        )\n",
    "\n",
    "\n",
    "    def _make_transpose_layer(self, block, planes, blocks, stride=1):\n",
    "\n",
    "        upsample = None\n",
    "        if stride != 1:\n",
    "            upsample = nn.Sequential(\n",
    "                nn.ConvTranspose2d(self.inplanes, planes, kernel_size=2, stride=stride, padding=0, bias=False),\n",
    "                nn.BatchNorm2d(planes),\n",
    "            )\n",
    "        elif self.inplanes != planes:\n",
    "            upsample = nn.Sequential(\n",
    "                nn.Conv2d(self.inplanes, planes, kernel_size=1, stride=stride, padding=0, bias=False),\n",
    "                nn.BatchNorm2d(planes),\n",
    "            )\n",
    "\n",
    "        for m in upsample.modules():  # 遍历卷积层 初始化权重\n",
    "            if isinstance(m, nn.ConvTranspose2d):\n",
    "                nn.init.xavier_uniform_(m.weight.data)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "        layers = []\n",
    "\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, self.inplanes))\n",
    "\n",
    "        layers.append(block(self.inplanes, planes, stride, upsample))\n",
    "        self.inplanes = planes\n",
    "\n",
    "        return nn.Sequential(*layers)  # 解包传参\n",
    "\n",
    "    def evaluate_purity(self, pred, rgb_feat=None, thermal_feat=None):\n",
    "    # 1. 基础纯净度：基于预测结果的熵\n",
    "        pred_probs = F.softmax(pred, dim=1)\n",
    "        pred_entropy = -torch.sum(pred_probs * torch.log(pred_probs + 1e-10), dim=1)\n",
    "        base_purity = 1 - pred_entropy.mean()  # 基础纯净度\n",
    "\n",
    "        rgb_purity = thermal_purity = base_purity\n",
    "\n",
    "        if rgb_feat is not None:\n",
    "            rgb_grad = torch.mean(torch.abs(rgb_feat[:, :, :-1] - rgb_feat[:, :, 1:]))\n",
    "            rgb_purity = base_purity * (1 + 0.5 * torch.sigmoid(rgb_grad - 0.5))\n",
    "\n",
    "        if thermal_feat is not None:\n",
    "            thermal_mean = torch.mean(thermal_feat)\n",
    "            thermal_std = torch.std(thermal_feat)\n",
    "            thermal_snr = thermal_mean / (thermal_std + 1e-10)\n",
    "            thermal_purity = base_purity * (1 + 0.3 * torch.tanh(thermal_snr))\n",
    "\n",
    "        return rgb_purity, thermal_purity\n",
    "\n",
    "    def forward(self, input, verbose=False, update_prior=False):\n",
    "\n",
    "        rgb = input[:, :3]\n",
    "        thermal = input[:, 3:]\n",
    "\n",
    "        # rgb = self.fusion1st(rgb, thermal)\n",
    "\n",
    "        if verbose: print(\"rgb.size() original: \", rgb.size())  # (480, 640)\n",
    "        if verbose: print(\"thermal.size() original: \", thermal.size())  # (480, 640)\n",
    "\n",
    "        rgb = self.encoder_rgb_conv1(rgb)\n",
    "        if verbose: print(\"rgb.size() after conv1: \", rgb.size())  # (240, 320)\n",
    "        rgb = self.encoder_rgb_bn1(rgb)\n",
    "        if verbose: print(\"rgb.size() after bn1: \", rgb.size())  # (240, 320)\n",
    "        rgb = self.encoder_rgb_relu(rgb)\n",
    "        if verbose: print(\"rgb.size() after relu: \", rgb.size())  # (240, 320)\n",
    "\n",
    "        thermal = self.encoder_thermal_conv1(thermal)\n",
    "        if verbose: print(\"thermal.size() after conv1: \", thermal.size())  # (240, 320)\n",
    "        thermal = self.encoder_thermal_bn1(thermal)\n",
    "        if verbose: print(\"thermal.size() after bn1: \", thermal.size())  # (240, 320)\n",
    "        thermal = self.encoder_thermal_relu(thermal)\n",
    "        if verbose: print(\"thermal.size() after relu: \", thermal.size())  # (240, 320)\n",
    "\n",
    "        # rgb = rgb + thermal\n",
    "        # rgb = self.fusion1(rgb, thermal) * self.fusion1.rgb_confidence.sigmoid()  # 融合一次\n",
    "\n",
    "        rgb = self.encoder_rgb_maxpool(rgb)\n",
    "        if verbose: print(\"rgb.size() after maxpool: \", rgb.size())  # (120, 160)\n",
    "\n",
    "        thermal = self.encoder_thermal_maxpool(thermal)\n",
    "        if verbose: print(\"thermal.size() after maxpool: \", thermal.size())  # (120, 160)\n",
    "\n",
    "        rgb = self.encoder_rgb_layer1(rgb)\n",
    "        if verbose: print(\"rgb.size() after layer1: \", rgb.size())  # (120, 160)\n",
    "        thermal = self.encoder_thermal_layer1(thermal)\n",
    "        if verbose: print(\"thermal.size() after layer1: \", thermal.size())  # (120, 160)\n",
    "\n",
    "        # rgb = self.fusion2(rgb, thermal) * self.fusion2.rgb_confidence.sigmoid()\n",
    "        # rgb = rgb + thermal\n",
    "\n",
    "        rgb = self.encoder_rgb_layer2(rgb)\n",
    "        if verbose: print(\"rgb.size() after layer2: \", rgb.size())  # (60, 80)\n",
    "        thermal = self.encoder_thermal_layer2(thermal)\n",
    "        if verbose: print(\"thermal.size() after layer2: \", thermal.size())  # (60, 80)\n",
    "\n",
    "        # rgb = self.fusion3(rgb, thermal)\n",
    "        rgb = rgb + thermal\n",
    "\n",
    "\n",
    "        rgb = self.encoder_rgb_layer3(rgb)\n",
    "        if verbose: print(\"rgb.size() after layer3: \", rgb.size())  # (30, 40)\n",
    "        thermal = self.encoder_thermal_layer3(thermal)\n",
    "        if verbose: print(\"thermal.size() after layer3: \", thermal.size())  # (30, 40)\n",
    "\n",
    "        # rgb = self.fusion4(rgb, thermal)\n",
    "        rgb = rgb + thermal\n",
    "        ######################################################################\n",
    "\n",
    "        rgb = self.encoder_rgb_layer4(rgb)\n",
    "        if verbose: print(\"rgb.size() after layer4: \", rgb.size())  # (15, 20)\n",
    "        thermal = self.encoder_thermal_layer4(thermal)\n",
    "        if verbose: print(\"thermal.size() after layer4: \", thermal.size())  # (15, 20)\n",
    "\n",
    "        fuse = rgb + thermal\n",
    "\n",
    "        ######################################################################\n",
    "\n",
    "        # decoder\n",
    "\n",
    "        fuse = self.deconv1(fuse)\n",
    "        if verbose: print(\"fuse after deconv1: \", fuse.size())  # (30, 40)\n",
    "        fuse = self.deconv2(fuse)\n",
    "        if verbose: print(\"fuse after deconv2: \", fuse.size())  # (60, 80)\n",
    "        fuse = self.deconv3(fuse)\n",
    "        if verbose: print(\"fuse after deconv3: \", fuse.size())  # (120, 160)\n",
    "        fuse = self.deconv4(fuse)\n",
    "        if verbose: print(\"fuse after deconv4: \", fuse.size())  # (240, 320)\n",
    "        fuse = self.deconv5(fuse)\n",
    "        if verbose: print(\"fuse after deconv5: \", fuse.size())  # (480, 640)\n",
    "\n",
    "        return fuse\n",
    "\n",
    "class TransBottleneck(nn.Module):\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, upsample=None):\n",
    "        super(TransBottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        if upsample is not None and stride != 1:\n",
    "            self.conv3 = nn.ConvTranspose2d(planes, planes, kernel_size=2, stride=stride, padding=0, bias=False)\n",
    "        else:\n",
    "            self.conv3 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "\n",
    "        self.bn3 = nn.BatchNorm2d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.upsample = upsample\n",
    "        self.stride = stride\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.xavier_uniform_(m.weight.data)\n",
    "            elif isinstance(m, nn.ConvTranspose2d):\n",
    "                nn.init.xavier_uniform_(m.weight.data)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def forward(self, x):  # 残差连接\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.upsample is not None:\n",
    "            residual = self.upsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测试运行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accomplished！\n"
     ]
    }
   ],
   "source": [
    "def unit_test():\n",
    "    num_minibatch = 1\n",
    "    rgb = torch.randn(num_minibatch, 3, 480, 640)\n",
    "    thermal = torch.randn(num_minibatch, 1, 480, 640)\n",
    "    rtf_net = RTFNet(9)\n",
    "    input = torch.cat((rgb, thermal), dim=1)\n",
    "    rtf_net(input)\n",
    "    # print('The model: ', rtf_net.modules)\n",
    "    print('Accomplished！')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    unit_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='Train with pytorch')\n",
    "parser.add_argument('--model_name', '-m', type=str, default='RTFNet')\n",
    "#batch_size: RTFNet-152: 2; RTFNet-101: 2; RTFNet-50: 3; RTFNet-34: 10; RTFNet-18: 15;\n",
    "parser.add_argument('--batch_size', '-b', type=int, default=2)\n",
    "parser.add_argument('--lr_start', '-ls', type=float, default=0.01)\n",
    "parser.add_argument('--gpu', '-g', type=int, default=0)\n",
    "#############################################################################################\n",
    "parser.add_argument('--lr_decay', '-ld', type=float, default=0.95)\n",
    "parser.add_argument('--epoch_max', '-em', type=int, default=1) # please stop training mannully\n",
    "parser.add_argument('--epoch_from', '-ef', type=int, default=0)\n",
    "parser.add_argument('--num_workers', '-j', type=int, default=2)\n",
    "parser.add_argument('--n_class', '-nc', type=int, default=9)\n",
    "parser.add_argument('--data_dir', '-dr', type=str, default='./dataset/')\n",
    "# 替换原来的 parse_args() 调用\n",
    "args, unknown = parser.parse_known_args()  # 自动忽略未定义的参数\n",
    "args.file_name = \"final.pth\"\n",
    "#############################################################################################\n",
    "\n",
    "augmentation_methods = [\n",
    "    RandomFlip(prob=0.5),\n",
    "    RandomCrop(crop_rate=0.1, prob=1.0),\n",
    "    # RandomCropOut(crop_rate=0.2, prob=1.0),\n",
    "    # RandomBrightness(bright_range=0.15, prob=0.9),\n",
    "    # RandomNoise(noise_range=5, prob=0.9),\n",
    "]\n",
    "writer = SummaryWriter()\n",
    "def train(epo, model, train_loader, optimizer):\n",
    "    model.train()\n",
    "    for it, (images, labels, names) in enumerate(train_loader):\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        images = Variable(images).to(device)\n",
    "        labels = Variable(labels).to(device)\n",
    "        start_t = time.time() # time.time() returns the current time\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(images)\n",
    "        loss = F.cross_entropy(logits, labels)  # Note that the cross_entropy function has already include the softmax function\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_this_epo=0\n",
    "        for param_group in optimizer.param_groups:\n",
    "            lr_this_epo = param_group['lr']\n",
    "        print('Train: %s, epo %s/%s, iter %s/%s, lr %.8f, %.2f img/sec, loss %.4f, time %s' \\\n",
    "            % (args.model_name, epo, args.epoch_max, it+1, len(train_loader), lr_this_epo, len(names)/(time.time()-start_t), float(loss),\n",
    "              datetime.datetime.now().replace(microsecond=0)-start_datetime))\n",
    "        if accIter['train'] % 1 == 0:\n",
    "            writer.add_scalar('Train/loss', loss, accIter['train'])\n",
    "        view_figure = True # note that I have not colorized the GT and predictions here\n",
    "        if accIter['train'] % 500 == 0:\n",
    "            if view_figure:\n",
    "                input_rgb_images = vutils.make_grid(images[:,:3], nrow=8, padding=10) # can only display 3-channel images, so images[:,:3]\n",
    "                writer.add_image('Train/input_rgb_images', input_rgb_images, accIter['train'])\n",
    "                scale = max(1, 255//args.n_class) # label (0,1,2..) is invisable, multiply a constant for visualization\n",
    "                groundtruth_tensor = labels.unsqueeze(1) * scale  # mini_batch*480*640 -> mini_batch*1*480*640\n",
    "                groundtruth_tensor = torch.cat((groundtruth_tensor, groundtruth_tensor, groundtruth_tensor), 1)  # change to 3-channel for visualization\n",
    "                groudtruth_images = vutils.make_grid(groundtruth_tensor, nrow=8, padding=10)\n",
    "                writer.add_image('Train/groudtruth_images', groudtruth_images, accIter['train'])\n",
    "                predicted_tensor = logits.argmax(1).unsqueeze(1) * scale # mini_batch*args.n_class*480*640 -> mini_batch*480*640 -> mini_batch*1*480*640\n",
    "                predicted_tensor = torch.cat((predicted_tensor, predicted_tensor, predicted_tensor),1) # change to 3-channel for visualization, mini_batch*1*480*640\n",
    "                predicted_images = vutils.make_grid(predicted_tensor, nrow=8, padding=10)\n",
    "                writer.add_image('Train/predicted_images', predicted_images, accIter['train'])\n",
    "        accIter['train'] = accIter['train'] + 1\n",
    "\n",
    "\n",
    "def validation(epo, model, val_loader):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for it, (images, labels, names) in enumerate(val_loader):\n",
    "            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "            images = Variable(images).to(device)\n",
    "            labels = Variable(labels).to(device)\n",
    "            start_t = time.time() # time.time() returns the current time\n",
    "            logits = model(images)\n",
    "            loss = F.cross_entropy(logits, labels)  # Note that the cross_entropy function has already include the softmax function\n",
    "            print('Val: %s, epo %s/%s, iter %s/%s, %.2f img/sec, loss %.4f, time %s' \\\n",
    "                  % (args.model_name, epo, args.epoch_max, it + 1, len(val_loader), len(names)/(time.time()-start_t), float(loss),\n",
    "                    datetime.datetime.now().replace(microsecond=0)-start_datetime))\n",
    "            if accIter['val'] % 1 == 0:\n",
    "                writer.add_scalar('Validation/loss', loss, accIter['val'])\n",
    "            view_figure = False  # note that I have not colorized the GT and predictions here\n",
    "            if accIter['val'] % 100 == 0:\n",
    "                if view_figure:\n",
    "                    input_rgb_images = vutils.make_grid(images[:, :3], nrow=8, padding=10)  # can only display 3-channel images, so images[:,:3]\n",
    "                    writer.add_image('Validation/input_rgb_images', input_rgb_images, accIter['val'])\n",
    "                    scale = max(1, 255 // args.n_class)  # label (0,1,2..) is invisable, multiply a constant for visualization\n",
    "                    groundtruth_tensor = labels.unsqueeze(1) * scale  # mini_batch*480*640 -> mini_batch*1*480*640\n",
    "                    groundtruth_tensor = torch.cat((groundtruth_tensor, groundtruth_tensor, groundtruth_tensor), 1)  # change to 3-channel for visualization\n",
    "                    groudtruth_images = vutils.make_grid(groundtruth_tensor, nrow=8, padding=10)\n",
    "                    writer.add_image('Validation/groudtruth_images', groudtruth_images, accIter['val'])\n",
    "                    predicted_tensor = logits.argmax(1).unsqueeze(1)*scale  # mini_batch*args.n_class*480*640 -> mini_batch*480*640 -> mini_batch*1*480*640\n",
    "                    predicted_tensor = torch.cat((predicted_tensor, predicted_tensor, predicted_tensor), 1)  # change to 3-channel for visualization, mini_batch*1*480*640\n",
    "                    predicted_images = vutils.make_grid(predicted_tensor, nrow=8, padding=10)\n",
    "                    writer.add_image('Validation/predicted_images', predicted_images, accIter['val'])\n",
    "            accIter['val'] += 1\n",
    "\n",
    "def testing(epo, model, test_loader):\n",
    "    model.eval()\n",
    "    conf_total = np.zeros((args.n_class, args.n_class))\n",
    "    label_list = [\"unlabeled\", \"car\", \"person\", \"bike\", \"curve\", \"car_stop\", \"guardrail\", \"color_cone\", \"bump\"]\n",
    "    testing_results_file = os.path.join(weight_dir, 'testing_results_file.txt')\n",
    "    with torch.no_grad():\n",
    "        for it, (images, labels, names) in enumerate(test_loader):\n",
    "            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "            images = Variable(images).to(device)\n",
    "            labels = Variable(labels).to(device)\n",
    "            logits = model(images)\n",
    "            label = labels.cpu().numpy().squeeze().flatten()\n",
    "            prediction = logits.argmax(1).cpu().numpy().squeeze().flatten() # prediction and label are both 1-d array, size: minibatch*640*480\n",
    "            conf = confusion_matrix(y_true=label, y_pred=prediction, labels=[0,1,2,3,4,5,6,7,8]) # conf is args.n_class*args.n_class matrix, vertical axis: groundtruth, horizontal axis: prediction\n",
    "            conf_total += conf\n",
    "            print('Test: %s, epo %s/%s, iter %s/%s, time %s' % (args.model_name, epo, args.epoch_max, it+1, len(test_loader),\n",
    "                 datetime.datetime.now().replace(microsecond=0)-start_datetime))\n",
    "    precision, recall, IoU = compute_results(conf_total)\n",
    "    writer.add_scalar('Test/average_precision',precision.mean(), epo)\n",
    "    writer.add_scalar('Test/average_recall', recall.mean(), epo)\n",
    "    writer.add_scalar('Test/average_IoU', IoU.mean(), epo)\n",
    "    for i in range(len(precision)):\n",
    "        writer.add_scalar(\"Test(class)/precision_class_%s\" % label_list[i], precision[i], epo)\n",
    "        writer.add_scalar(\"Test(class)/recall_class_%s\"% label_list[i], recall[i],epo)\n",
    "        writer.add_scalar('Test(class)/Iou_%s'% label_list[i], IoU[i], epo)\n",
    "    if epo==0:\n",
    "        with open(testing_results_file, 'w') as f:\n",
    "            f.write(\"# %s, initial lr: %s, batch size: %s, date: %s \\n\" %(args.model_name, args.lr_start, args.batch_size, datetime.date.today()))\n",
    "            f.write(\"# epoch: unlabeled, car, person, bike, curve, car_stop, guardrail, color_cone, bump, average(nan_to_num). (Acc %, IoU %)\\n\")\n",
    "    with open(testing_results_file, 'a') as f:\n",
    "        f.write(str(epo)+': ')\n",
    "        for i in range(len(precision)):\n",
    "            f.write('%0.4f, %0.4f, ' % (100*recall[i], 100*IoU[i]))\n",
    "        f.write('%0.4f, %0.4f\\n' % (100*np.mean(np.nan_to_num(recall)), 100*np.mean(np.nan_to_num(IoU))))\n",
    "    print('saving testing results.')\n",
    "    with open(testing_results_file, \"r\") as file:\n",
    "        writer.add_text('testing_results', file.read().replace('\\n', '  \\n'), epo)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    model = eval(args.model_name)(n_class=args.n_class)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)  # 将模型移动到 CPU\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr_start, momentum=0.9, weight_decay=0.0005)\n",
    "    scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=args.lr_decay, last_epoch=-1)\n",
    "\n",
    "    # preparing folders\n",
    "    weight_dir = os.path.join(\"./runs\", args.model_name)\n",
    "    print('from epoch %d / %s' % (args.epoch_from, args.epoch_max))\n",
    "    print('weight will be saved in: %s' % weight_dir)\n",
    "\n",
    "    train_dataset = MF_dataset(data_dir=args.data_dir, split='train', transform=augmentation_methods)\n",
    "    val_dataset  = MF_dataset(data_dir=args.data_dir, split='val')\n",
    "    test_dataset = MF_dataset(data_dir=args.data_dir, split='test')\n",
    "\n",
    "    train_loader  = DataLoader(\n",
    "        dataset     = train_dataset,\n",
    "        batch_size  = args.batch_size,\n",
    "        shuffle     = True,\n",
    "        num_workers = args.num_workers,\n",
    "        pin_memory  = True,\n",
    "        drop_last   = False\n",
    "    )\n",
    "    val_loader  = DataLoader(\n",
    "        dataset     = val_dataset,\n",
    "        batch_size  = args.batch_size,\n",
    "        shuffle     = False,\n",
    "        num_workers = args.num_workers,\n",
    "        pin_memory  = True,\n",
    "        drop_last   = False\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        dataset      = test_dataset,\n",
    "        batch_size   = args.batch_size,\n",
    "        shuffle      = False,\n",
    "        num_workers  = args.num_workers,\n",
    "        pin_memory   = True,\n",
    "        drop_last    = False\n",
    "    )\n",
    "    start_datetime = datetime.datetime.now().replace(microsecond=0)\n",
    "    accIter = {'train': 0, 'val': 0}\n",
    "    for epo in range(args.epoch_from, args.epoch_max):\n",
    "        print('\\ntrain %s, epo #%s begin...' % (args.model_name, epo))\n",
    "        train(epo, model, train_loader, optimizer)\n",
    "        validation(epo, model, val_loader)\n",
    "\n",
    "    checkpoint_model_file = os.path.join(weight_dir, str(epo) + '.pth')\n",
    "    print('saving check point %s: ' % checkpoint_model_file)\n",
    "    torch.save(model.state_dict(), checkpoint_model_file)\n",
    "\n",
    "    testing(epo, model, test_loader) # testing is just for your reference, you can comment this line during training\n",
    "    scheduler.step() # if using pytorch 1.1 or above, please put this statement here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 测试运行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frame 1/20, ('01500D',), time cost: 1381.78 ms, demo result saved.\n",
      "frame 2/20, ('01501D',), time cost: 1262.53 ms, demo result saved.\n",
      "frame 3/20, ('01502D',), time cost: 1011.41 ms, demo result saved.\n",
      "frame 4/20, ('01503D',), time cost: 927.35 ms, demo result saved.\n",
      "frame 5/20, ('01504D',), time cost: 919.30 ms, demo result saved.\n",
      "frame 6/20, ('01505D',), time cost: 971.62 ms, demo result saved.\n",
      "frame 7/20, ('01506D',), time cost: 959.92 ms, demo result saved.\n",
      "frame 8/20, ('01507D',), time cost: 954.81 ms, demo result saved.\n",
      "frame 9/20, ('01508D',), time cost: 971.74 ms, demo result saved.\n",
      "frame 10/20, ('01509D',), time cost: 1004.99 ms, demo result saved.\n",
      "frame 11/20, ('01200N',), time cost: 1063.20 ms, demo result saved.\n",
      "frame 12/20, ('01201N',), time cost: 1006.08 ms, demo result saved.\n",
      "frame 13/20, ('01202N',), time cost: 980.92 ms, demo result saved.\n",
      "frame 14/20, ('01203N',), time cost: 983.25 ms, demo result saved.\n",
      "frame 15/20, ('01204N',), time cost: 1000.63 ms, demo result saved.\n",
      "frame 16/20, ('01205N',), time cost: 1051.80 ms, demo result saved.\n",
      "frame 17/20, ('01206N',), time cost: 1010.88 ms, demo result saved.\n",
      "frame 18/20, ('01207N',), time cost: 993.87 ms, demo result saved.\n",
      "frame 19/20, ('01208N',), time cost: 994.16 ms, demo result saved.\n",
      "frame 20/20, ('01209N',), time cost: 1028.12 ms, demo result saved.\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(description='Test with pytorch')\n",
    "\n",
    "parser.add_argument('--model_name', '-m', type=str, default='RTFNet')\n",
    "parser.add_argument('--weight_name', '-w', type=str, default='RTFNET_152') # RTFNet_152, RTFNet_50, please change the number of layers in the network file\n",
    "parser.add_argument('--file_name', '-fi', type=str, default='final.pth')\n",
    "parser.add_argument('--dataset_split', '-d', type=str, default='test') # test, test_day, test_night\n",
    "parser.add_argument('--img_height', '-ih', type=int, default=480)\n",
    "parser.add_argument('--img_width', '-iw', type=int, default=640)\n",
    "parser.add_argument('--num_workers', '-j', type=int, default=8)\n",
    "parser.add_argument('--n_class', '-nc', type=int, default=9)\n",
    "parser.add_argument('--data_dir', '-dr', type=str, default='./dataset/')\n",
    "parser.add_argument('--model_dir', '-wd', type=str, default='./weights_backup/')\n",
    "\n",
    "\n",
    "args = parser.parse_args()\n",
    "args.file_name = \"final.pth\"\n",
    "if __name__ == '__main__':\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model_dir = os.path.join(args.model_dir, args.weight_name)\n",
    "    # if os.path.exists(model_dir) is False:\n",
    "    #     sys.exit(\"the %s does not exit.\" %(model_dir))\n",
    "    model_file = os.path.join(model_dir, args.file_name)\n",
    "\n",
    "\n",
    "    conf_total = np.zeros((args.n_class, args.n_class))\n",
    "    model = eval(args.model_name)(n_class=args.n_class)\n",
    "    model = model.to(device)\n",
    "\n",
    "    pretrained_weight = torch.load(model_file, map_location='cpu', weights_only=True)\n",
    "    own_state = model.state_dict()\n",
    "    for name, param in pretrained_weight.items():\n",
    "        if name not in own_state:\n",
    "            continue\n",
    "        own_state[name].copy_(param)\n",
    "\n",
    "    batch_size = 1\n",
    "    test_dataset  = MF_dataset(data_dir=args.data_dir, split=args.dataset_split, input_h=args.img_height, input_w=args.img_width)\n",
    "    test_loader  = DataLoader(\n",
    "        dataset     = test_dataset,\n",
    "        batch_size  = batch_size,\n",
    "        shuffle     = False,\n",
    "        num_workers = args.num_workers,\n",
    "        pin_memory  = True,\n",
    "        drop_last   = False\n",
    "    )\n",
    "    ave_time_cost = 0.0\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for it, (images, labels, names) in enumerate(test_loader):\n",
    "            images = Variable(images).to(device)\n",
    "            labels = Variable(labels).to(device)\n",
    "            start_time = time.time()\n",
    "            logits = model(images)  # logits.size(): mini_batch*num_class*480*640\n",
    "            end_time = time.time()\n",
    "            if it>=5: # # ignore the first 5 frames\n",
    "                ave_time_cost += (end_time-start_time)\n",
    "            # convert tensor to numpy 1d array\n",
    "            label = labels.detach().cpu().numpy().squeeze().flatten()\n",
    "            prediction = logits.argmax(1).detach().cpu().numpy().squeeze().flatten() # prediction and label are both 1-d array, size: minibatch*640*480\n",
    "            # generate confusion matrix frame-by-frame\n",
    "            # conf = confusion_matrix(y_true=label, y_pred=prediction, labels=[0,1,2,3,4,5,6,7,8]) # conf is an n_class*n_class matrix, vertical axis: groundtruth, horizontal axis: prediction\n",
    "            # conf_total += conf\n",
    "            # save demo images\n",
    "            visualize(image_name=names, predictions=logits.argmax(1), weight_name=args.weight_name)\n",
    "            print(\"frame %d/%d, %s, time cost: %.2f ms, demo result saved.\"\n",
    "                  %(it+1, len(test_loader), names, (end_time-start_time)*1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1600x800 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "pic = \"1500D\"\n",
    "# 指定图像路径（例如测试集中的第一张图像）\n",
    "image_path = 'runs/Predict_RTFNET_152_0{}.png'.format(pic)  # 根据实际路径调整\n",
    "image_path1 = 'runs/Pred_RTFNet_152_0{}.png'.format(pic)\n",
    "plt.figure(figsize=(16, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "img = plt.imread(image_path)\n",
    "plt.imshow(img)\n",
    "plt.axis('off')  # 隐藏坐标轴\n",
    "plt.title(\"Modified\")\n",
    "plt.subplot(1, 2, 2)\n",
    "img1 = plt.imread(image_path1)\n",
    "plt.imshow(img1)\n",
    "plt.axis('off')  # 隐藏坐标轴\n",
    "plt.title(\"Original\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multimodel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
