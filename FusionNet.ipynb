{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 导库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, argparse, time, datetime, sys, shutil, stat, torch\n",
    "import torchvision.models as models\n",
    "import torchvision.utils as vutils\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "from util.MF_dataset import MF_dataset \n",
    "from util.util import compute_results, visualize\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from scipy.io import savemat \n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from util.augmentation import RandomFlip, RandomCrop, RandomCropOut, RandomBrightness, RandomNoise\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision.models import ResNet152_Weights, ResNet50_Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型搭建"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 注意力机制"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionFusion(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.attetion = nn.Sequential(\n",
    "            nn.Conv2d(channels * 2, channels // 2, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(channels // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(channels // 2, 2, kernel_size=3, padding=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.rgb_confidence = nn.Parameter(torch.tensor(1.0))\n",
    "        self.thermal_confidence = nn.Parameter(torch.tensor(1.0))\n",
    "\n",
    "        self.register_buffer('rgb_prior', torch.ones(1))\n",
    "        self.register_buffer('thermal_prior', torch.ones(1))\n",
    "        self.momentum = 0.9\n",
    "    \n",
    "    def update(self, rgb_purity, thermal_purity):\n",
    "        self.rgb_prior = self.momentum * self.rgb_prior + (1-self.momentum)* rgb_purity\n",
    "        self.thermal_prior = self.momentum * self.thermal_prior + (1-self.momentum) * thermal_purity\n",
    "        self.rgb_prior.data.clamp_(0.1, 1.0)\n",
    "        self.thermal_prior.data.clamp_(0.1, 1.0)\n",
    "\n",
    "    def forward(self, rgb_feat, thermal_feat):\n",
    "        combined = torch.cat([rgb_feat, thermal_feat], dim=1)\n",
    "        attention_weights = self.attention(combined)  # [B,2,H,W]\n",
    "        \n",
    "        # 将置信度参数转换为与attention_weights相同的维度\n",
    "        rgb_conf = torch.sigmoid(self.rgb_confidence).view(1,1,1,1)  # [1,1,1,1]\n",
    "        thermal_conf = torch.sigmoid(self.thermal_confidence).view(1,1,1,1)\n",
    "        \n",
    "        # 动态调整权重（关键修改点）\n",
    "        adjusted_weights = torch.cat([\n",
    "            attention_weights[:,0:1] * rgb_conf * self.rgb_prior,\n",
    "            attention_weights[:,1:2] * thermal_conf * self.thermal_prior\n",
    "        ], dim=1)\n",
    "        \n",
    "        # 重新归一化\n",
    "        norm_weights = adjusted_weights / (adjusted_weights.sum(dim=1, keepdim=True) + 1e-8)\n",
    "        \n",
    "        return rgb_feat * norm_weights[:,0:1] + thermal_feat * norm_weights[:,1:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 网络定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RTFNet(nn.Module):\n",
    "\n",
    "    def __init__(self, n_class):\n",
    "        super(RTFNet, self).__init__()  # 初始化\n",
    "\n",
    "        self.num_resnet_layers = 152\n",
    "\n",
    "        if self.num_resnet_layers == 50:\n",
    "            resnet_raw_model1 = models.resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "            resnet_raw_model2 = models.resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "            self.inplanes = 2048\n",
    "        elif self.num_resnet_layers == 152:\n",
    "            resnet_raw_model1 = models.resnet152(weights=ResNet152_Weights.DEFAULT)\n",
    "            resnet_raw_model2 = models.resnet152(weights=ResNet152_Weights.DEFAULT)\n",
    "            self.inplanes = 2048\n",
    "\n",
    "        ########  Thermal ENCODER  ########\n",
    "\n",
    "        self.encoder_thermal_conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.encoder_thermal_conv1.weight.data = torch.unsqueeze(torch.mean(resnet_raw_model1.conv1.weight.data, dim=1),\n",
    "                                                                 dim=1)\n",
    "        self.encoder_thermal_bn1 = resnet_raw_model1.bn1\n",
    "        self.encoder_thermal_relu = resnet_raw_model1.relu\n",
    "        self.encoder_thermal_maxpool = resnet_raw_model1.maxpool\n",
    "        self.encoder_thermal_layer1 = resnet_raw_model1.layer1\n",
    "        self.encoder_thermal_layer2 = resnet_raw_model1.layer2\n",
    "        self.encoder_thermal_layer3 = resnet_raw_model1.layer3\n",
    "        self.encoder_thermal_layer4 = resnet_raw_model1.layer4\n",
    "\n",
    "        ########  RGB ENCODER  ########\n",
    "\n",
    "        self.encoder_rgb_conv1 = resnet_raw_model2.conv1\n",
    "        self.encoder_rgb_bn1 = resnet_raw_model2.bn1\n",
    "        self.encoder_rgb_relu = resnet_raw_model2.relu\n",
    "        self.encoder_rgb_maxpool = resnet_raw_model2.maxpool\n",
    "        self.encoder_rgb_layer1 = resnet_raw_model2.layer1\n",
    "        self.encoder_rgb_layer2 = resnet_raw_model2.layer2\n",
    "        self.encoder_rgb_layer3 = resnet_raw_model2.layer3\n",
    "        self.encoder_rgb_layer4 = resnet_raw_model2.layer4\n",
    "\n",
    "        ########  Fusion   ########\n",
    "        self.fusion1 = AttentionFusion(64)\n",
    "        self.fusion2 = AttentionFusion(256)\n",
    "        self.fusion3 = AttentionFusion(512)\n",
    "        self.fusion4 = AttentionFusion(1024)\n",
    "        self.fusion5 = AttentionFusion(2048)\n",
    "        self.fusion1st = AttentionFusion(2)\n",
    "\n",
    "        self.register_buffer('last_rgb_pred', None)\n",
    "        self.register_buffer('last_thermal_feat', None)\n",
    "        self.register_buffer('last_purity', torch.zeros(2))\n",
    "\n",
    "        ########  DECODER  ########\n",
    "\n",
    "        self.deconv1 = self._make_transpose_layer(TransBottleneck, self.inplanes // 2, 2,\n",
    "                                                  stride=2)  # using // for python 3.6\n",
    "        self.deconv2 = self._make_transpose_layer(TransBottleneck, self.inplanes // 2, 2,\n",
    "                                                  stride=2)  # using // for python 3.6\n",
    "        self.deconv3 = self._make_transpose_layer(TransBottleneck, self.inplanes // 2, 2,\n",
    "                                                  stride=2)  # using // for python 3.6\n",
    "        self.deconv4 = self._make_transpose_layer(TransBottleneck, self.inplanes // 2, 2,\n",
    "                                                  stride=2)  # using // for python 3.6\n",
    "        self.deconv5 = self._make_transpose_layer(TransBottleneck, n_class, 2, stride=2)\n",
    "\n",
    "        self.side_conv1 = nn.Conv2d(1024, n_class, kernel_size=1)\n",
    "        self.side_conv2 = nn.Conv2d(512, n_class, kernel_size=1)\n",
    "        self.side_conv3 = nn.Conv2d(256, n_class, kernel_size=1)\n",
    "        self.side_conv4 = nn.Conv2d(128, n_class, kernel_size=1)\n",
    "\n",
    "        self.detail_enhance = nn.Sequential(\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, n_class, kernel_size=1)\n",
    "        )\n",
    "\n",
    "\n",
    "    def _make_transpose_layer(self, block, planes, blocks, stride=1):\n",
    "\n",
    "        upsample = None\n",
    "        if stride != 1:\n",
    "            upsample = nn.Sequential(\n",
    "                nn.ConvTranspose2d(self.inplanes, planes, kernel_size=2, stride=stride, padding=0, bias=False),\n",
    "                nn.BatchNorm2d(planes),\n",
    "            )\n",
    "        elif self.inplanes != planes:\n",
    "            upsample = nn.Sequential(\n",
    "                nn.Conv2d(self.inplanes, planes, kernel_size=1, stride=stride, padding=0, bias=False),\n",
    "                nn.BatchNorm2d(planes),\n",
    "            )\n",
    "\n",
    "        for m in upsample.modules():  # 遍历卷积层 初始化权重\n",
    "            if isinstance(m, nn.ConvTranspose2d):\n",
    "                nn.init.xavier_uniform_(m.weight.data)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "        layers = []\n",
    "\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, self.inplanes))\n",
    "\n",
    "        layers.append(block(self.inplanes, planes, stride, upsample))\n",
    "        self.inplanes = planes\n",
    "\n",
    "        return nn.Sequential(*layers)  # 解包传参\n",
    "\n",
    "    def evaluate_purity(self, pred, rgb_feat=None, thermal_feat=None):\n",
    "    # 1. 基础纯净度：基于预测结果的熵\n",
    "        pred_probs = F.softmax(pred, dim=1)\n",
    "        pred_entropy = -torch.sum(pred_probs * torch.log(pred_probs + 1e-10), dim=1)\n",
    "        base_purity = 1 - pred_entropy.mean()  # 基础纯净度\n",
    "        \n",
    "        rgb_purity = thermal_purity = base_purity\n",
    "        \n",
    "        if rgb_feat is not None:\n",
    "            rgb_grad = torch.mean(torch.abs(rgb_feat[:, :, :-1] - rgb_feat[:, :, 1:]))  \n",
    "            rgb_purity = base_purity * (1 + 0.5 * torch.sigmoid(rgb_grad - 0.5)) \n",
    "            \n",
    "        if thermal_feat is not None:\n",
    "            thermal_mean = torch.mean(thermal_feat)\n",
    "            thermal_std = torch.std(thermal_feat)\n",
    "            thermal_snr = thermal_mean / (thermal_std + 1e-10)\n",
    "            thermal_purity = base_purity * (1 + 0.3 * torch.tanh(thermal_snr))\n",
    "        \n",
    "        return rgb_purity, thermal_purity\n",
    "    \n",
    "    def forward(self, input, verbose=False, update_prior=False):\n",
    "\n",
    "        rgb = input[:, :3]\n",
    "        thermal = input[:, 3:]\n",
    "        \n",
    "        # rgb = self.fusion1st(rgb, thermal)\n",
    "\n",
    "        if verbose: print(\"rgb.size() original: \", rgb.size())  # (480, 640)\n",
    "        if verbose: print(\"thermal.size() original: \", thermal.size())  # (480, 640)\n",
    "\n",
    "        rgb = self.encoder_rgb_conv1(rgb)\n",
    "        if verbose: print(\"rgb.size() after conv1: \", rgb.size())  # (240, 320)\n",
    "        rgb = self.encoder_rgb_bn1(rgb)\n",
    "        if verbose: print(\"rgb.size() after bn1: \", rgb.size())  # (240, 320)\n",
    "        rgb = self.encoder_rgb_relu(rgb)\n",
    "        if verbose: print(\"rgb.size() after relu: \", rgb.size())  # (240, 320)\n",
    "\n",
    "        thermal = self.encoder_thermal_conv1(thermal)\n",
    "        if verbose: print(\"thermal.size() after conv1: \", thermal.size())  # (240, 320)\n",
    "        thermal = self.encoder_thermal_bn1(thermal)\n",
    "        if verbose: print(\"thermal.size() after bn1: \", thermal.size())  # (240, 320)\n",
    "        thermal = self.encoder_thermal_relu(thermal)\n",
    "        if verbose: print(\"thermal.size() after relu: \", thermal.size())  # (240, 320)\n",
    "\n",
    "        # rgb = rgb + thermal\n",
    "        # rgb = self.fusion1(rgb, thermal) * self.fusion1.rgb_confidence.sigmoid()  # 融合一次\n",
    "\n",
    "        rgb = self.encoder_rgb_maxpool(rgb)\n",
    "        if verbose: print(\"rgb.size() after maxpool: \", rgb.size())  # (120, 160)\n",
    "\n",
    "        thermal = self.encoder_thermal_maxpool(thermal)\n",
    "        if verbose: print(\"thermal.size() after maxpool: \", thermal.size())  # (120, 160)\n",
    "\n",
    "        rgb = self.encoder_rgb_layer1(rgb)\n",
    "        if verbose: print(\"rgb.size() after layer1: \", rgb.size())  # (120, 160)\n",
    "        thermal = self.encoder_thermal_layer1(thermal)\n",
    "        if verbose: print(\"thermal.size() after layer1: \", thermal.size())  # (120, 160)\n",
    "\n",
    "        # rgb = self.fusion2(rgb, thermal) * self.fusion2.rgb_confidence.sigmoid()\n",
    "        # rgb = rgb + thermal\n",
    "\n",
    "        rgb = self.encoder_rgb_layer2(rgb)\n",
    "        if verbose: print(\"rgb.size() after layer2: \", rgb.size())  # (60, 80)\n",
    "        thermal = self.encoder_thermal_layer2(thermal)\n",
    "        if verbose: print(\"thermal.size() after layer2: \", thermal.size())  # (60, 80)\n",
    "\n",
    "        # rgb = self.fusion3(rgb, thermal)\n",
    "        rgb = rgb + thermal\n",
    "\n",
    "\n",
    "        rgb = self.encoder_rgb_layer3(rgb)\n",
    "        if verbose: print(\"rgb.size() after layer3: \", rgb.size())  # (30, 40)\n",
    "        thermal = self.encoder_thermal_layer3(thermal)\n",
    "        if verbose: print(\"thermal.size() after layer3: \", thermal.size())  # (30, 40)\n",
    "\n",
    "        # rgb = self.fusion4(rgb, thermal)\n",
    "        rgb = rgb + thermal\n",
    "        ######################################################################\n",
    "\n",
    "        rgb = self.encoder_rgb_layer4(rgb)\n",
    "        if verbose: print(\"rgb.size() after layer4: \", rgb.size())  # (15, 20)\n",
    "        thermal = self.encoder_thermal_layer4(thermal)\n",
    "        if verbose: print(\"thermal.size() after layer4: \", thermal.size())  # (15, 20)\n",
    "\n",
    "        fuse = rgb + thermal\n",
    "\n",
    "        ######################################################################\n",
    "\n",
    "        # decoder\n",
    "\n",
    "        fuse = self.deconv1(fuse)\n",
    "        if verbose: print(\"fuse after deconv1: \", fuse.size())  # (30, 40)\n",
    "        fuse = self.deconv2(fuse)\n",
    "        if verbose: print(\"fuse after deconv2: \", fuse.size())  # (60, 80)\n",
    "        fuse = self.deconv3(fuse)\n",
    "        if verbose: print(\"fuse after deconv3: \", fuse.size())  # (120, 160)\n",
    "        fuse = self.deconv4(fuse)\n",
    "        if verbose: print(\"fuse after deconv4: \", fuse.size())  # (240, 320)\n",
    "        fuse = self.deconv5(fuse)\n",
    "        if verbose: print(\"fuse after deconv5: \", fuse.size())  # (480, 640)\n",
    "\n",
    "        return fuse\n",
    "\n",
    "class TransBottleneck(nn.Module):\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, upsample=None):\n",
    "        super(TransBottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        if upsample is not None and stride != 1:\n",
    "            self.conv3 = nn.ConvTranspose2d(planes, planes, kernel_size=2, stride=stride, padding=0, bias=False)\n",
    "        else:\n",
    "            self.conv3 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "\n",
    "        self.bn3 = nn.BatchNorm2d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.upsample = upsample\n",
    "        self.stride = stride\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.xavier_uniform_(m.weight.data)\n",
    "            elif isinstance(m, nn.ConvTranspose2d):\n",
    "                nn.init.xavier_uniform_(m.weight.data)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def forward(self, x):  # 残差连接\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.upsample is not None:\n",
    "            residual = self.upsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测试运行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accomplished！\n"
     ]
    }
   ],
   "source": [
    "def unit_test():\n",
    "    num_minibatch = 1\n",
    "    rgb = torch.randn(num_minibatch, 3, 480, 640)\n",
    "    thermal = torch.randn(num_minibatch, 1, 480, 640)\n",
    "    rtf_net = RTFNet(9)\n",
    "    input = torch.cat((rgb, thermal), dim=1)\n",
    "    rtf_net(input)\n",
    "    # print('The model: ', rtf_net.modules)\n",
    "    print('Accomplished！')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    unit_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from epoch 0 / 1\n",
      "weight will be saved in: ./runs/RTFNet\n",
      "\n",
      "train RTFNet, epo #0 begin...\n",
      "Train: RTFNet, epo 0/1, iter 1/100, lr 0.01000000, 0.18 img/sec, loss 2.4623, time 0:00:13\n",
      "Train: RTFNet, epo 0/1, iter 2/100, lr 0.01000000, 0.16 img/sec, loss 2.4648, time 0:00:26\n",
      "Train: RTFNet, epo 0/1, iter 3/100, lr 0.01000000, 0.16 img/sec, loss 2.4273, time 0:00:38\n",
      "Train: RTFNet, epo 0/1, iter 4/100, lr 0.01000000, 0.17 img/sec, loss 2.3835, time 0:00:50\n",
      "Train: RTFNet, epo 0/1, iter 5/100, lr 0.01000000, 0.18 img/sec, loss 2.3430, time 0:01:01\n",
      "Train: RTFNet, epo 0/1, iter 6/100, lr 0.01000000, 0.17 img/sec, loss 2.2763, time 0:01:13\n",
      "Train: RTFNet, epo 0/1, iter 7/100, lr 0.01000000, 0.17 img/sec, loss 2.1921, time 0:01:25\n",
      "Train: RTFNet, epo 0/1, iter 8/100, lr 0.01000000, 0.17 img/sec, loss 2.1502, time 0:01:37\n",
      "Train: RTFNet, epo 0/1, iter 9/100, lr 0.01000000, 0.17 img/sec, loss 2.0831, time 0:01:49\n",
      "Train: RTFNet, epo 0/1, iter 10/100, lr 0.01000000, 0.16 img/sec, loss 1.9944, time 0:02:01\n",
      "Train: RTFNet, epo 0/1, iter 11/100, lr 0.01000000, 0.16 img/sec, loss 1.9424, time 0:02:14\n",
      "Train: RTFNet, epo 0/1, iter 12/100, lr 0.01000000, 0.18 img/sec, loss 1.8895, time 0:02:25\n",
      "Train: RTFNet, epo 0/1, iter 13/100, lr 0.01000000, 0.17 img/sec, loss 1.7900, time 0:02:37\n",
      "Train: RTFNet, epo 0/1, iter 14/100, lr 0.01000000, 0.17 img/sec, loss 1.7567, time 0:02:49\n",
      "Train: RTFNet, epo 0/1, iter 15/100, lr 0.01000000, 0.16 img/sec, loss 1.6908, time 0:03:01\n",
      "Train: RTFNet, epo 0/1, iter 16/100, lr 0.01000000, 0.17 img/sec, loss 1.6465, time 0:03:12\n",
      "Train: RTFNet, epo 0/1, iter 17/100, lr 0.01000000, 0.15 img/sec, loss 1.6505, time 0:03:25\n",
      "Train: RTFNet, epo 0/1, iter 18/100, lr 0.01000000, 0.16 img/sec, loss 1.7072, time 0:03:38\n",
      "Train: RTFNet, epo 0/1, iter 19/100, lr 0.01000000, 0.16 img/sec, loss 1.6009, time 0:03:51\n",
      "Train: RTFNet, epo 0/1, iter 20/100, lr 0.01000000, 0.15 img/sec, loss 1.4107, time 0:04:04\n",
      "Train: RTFNet, epo 0/1, iter 21/100, lr 0.01000000, 0.16 img/sec, loss 1.4147, time 0:04:16\n",
      "Train: RTFNet, epo 0/1, iter 22/100, lr 0.01000000, 0.16 img/sec, loss 1.3246, time 0:04:29\n",
      "Train: RTFNet, epo 0/1, iter 23/100, lr 0.01000000, 0.14 img/sec, loss 1.4148, time 0:04:44\n",
      "Train: RTFNet, epo 0/1, iter 24/100, lr 0.01000000, 0.16 img/sec, loss 1.3476, time 0:04:57\n",
      "Train: RTFNet, epo 0/1, iter 25/100, lr 0.01000000, 0.16 img/sec, loss 1.3354, time 0:05:09\n",
      "Train: RTFNet, epo 0/1, iter 26/100, lr 0.01000000, 0.16 img/sec, loss 1.4098, time 0:05:22\n",
      "Train: RTFNet, epo 0/1, iter 27/100, lr 0.01000000, 0.15 img/sec, loss 1.2010, time 0:05:35\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 175\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epo \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(args\u001b[38;5;241m.\u001b[39mepoch_from, args\u001b[38;5;241m.\u001b[39mepoch_max):\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mtrain \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m, epo #\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m begin...\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (args\u001b[38;5;241m.\u001b[39mmodel_name, epo))\n\u001b[0;32m--> 175\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    176\u001b[0m     validation(epo, model, val_loader)\n\u001b[1;32m    178\u001b[0m checkpoint_model_file \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(weight_dir, \u001b[38;5;28mstr\u001b[39m(epo) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.pth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[5], line 34\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(epo, model, train_loader, optimizer)\u001b[0m\n\u001b[1;32m     32\u001b[0m start_t \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;66;03m# time.time() returns the current time\u001b[39;00m\n\u001b[1;32m     33\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 34\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mcross_entropy(logits, labels)  \u001b[38;5;66;03m# Note that the cross_entropy function has already include the softmax function\u001b[39;00m\n\u001b[1;32m     36\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/multimodel/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/multimodel/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[3], line 207\u001b[0m, in \u001b[0;36mRTFNet.forward\u001b[0;34m(self, input, verbose, update_prior)\u001b[0m\n\u001b[1;32m    205\u001b[0m fuse \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeconv4(fuse)\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verbose: \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfuse after deconv4: \u001b[39m\u001b[38;5;124m\"\u001b[39m, fuse\u001b[38;5;241m.\u001b[39msize())  \u001b[38;5;66;03m# (240, 320)\u001b[39;00m\n\u001b[0;32m--> 207\u001b[0m fuse \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeconv5\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfuse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verbose: \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfuse after deconv5: \u001b[39m\u001b[38;5;124m\"\u001b[39m, fuse\u001b[38;5;241m.\u001b[39msize())  \u001b[38;5;66;03m# (480, 640)\u001b[39;00m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fuse\n",
      "File \u001b[0;32m/opt/anaconda3/envs/multimodel/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/multimodel/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/multimodel/lib/python3.12/site-packages/torch/nn/modules/container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/multimodel/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/multimodel/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[3], line 247\u001b[0m, in \u001b[0;36mTransBottleneck.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    244\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn1(out)\n\u001b[1;32m    245\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(out)\n\u001b[0;32m--> 247\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    248\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn2(out)\n\u001b[1;32m    249\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(out)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/multimodel/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/multimodel/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/multimodel/lib/python3.12/site-packages/torch/nn/modules/conv.py:554\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 554\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/multimodel/lib/python3.12/site-packages/torch/nn/modules/conv.py:549\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    538\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\n\u001b[1;32m    539\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[1;32m    540\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    547\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[1;32m    548\u001b[0m     )\n\u001b[0;32m--> 549\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    550\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\n\u001b[1;32m    551\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(description='Train with pytorch')\n",
    "parser.add_argument('--model_name', '-m', type=str, default='RTFNet')\n",
    "#batch_size: RTFNet-152: 2; RTFNet-101: 2; RTFNet-50: 3; RTFNet-34: 10; RTFNet-18: 15;\n",
    "parser.add_argument('--batch_size', '-b', type=int, default=2) \n",
    "parser.add_argument('--lr_start', '-ls', type=float, default=0.01)\n",
    "parser.add_argument('--gpu', '-g', type=int, default=0)\n",
    "#############################################################################################\n",
    "parser.add_argument('--lr_decay', '-ld', type=float, default=0.95)\n",
    "parser.add_argument('--epoch_max', '-em', type=int, default=1) # please stop training mannully \n",
    "parser.add_argument('--epoch_from', '-ef', type=int, default=0) \n",
    "parser.add_argument('--num_workers', '-j', type=int, default=8)\n",
    "parser.add_argument('--n_class', '-nc', type=int, default=9)\n",
    "parser.add_argument('--data_dir', '-dr', type=str, default='./dataset/')\n",
    "# 替换原来的 parse_args() 调用\n",
    "args, unknown = parser.parse_known_args()  # 自动忽略未定义的参数\n",
    "args.file_name = \"final.pth\" \n",
    "#############################################################################################\n",
    "\n",
    "augmentation_methods = [\n",
    "    RandomFlip(prob=0.5),\n",
    "    RandomCrop(crop_rate=0.1, prob=1.0),\n",
    "    # RandomCropOut(crop_rate=0.2, prob=1.0),\n",
    "    # RandomBrightness(bright_range=0.15, prob=0.9),\n",
    "    # RandomNoise(noise_range=5, prob=0.9),\n",
    "]\n",
    "writer = SummaryWriter()\n",
    "def train(epo, model, train_loader, optimizer):\n",
    "    model.train()\n",
    "    for it, (images, labels, names) in enumerate(train_loader):\n",
    "        images = Variable(images).to('cpu')\n",
    "        labels = Variable(labels).to('cpu')\n",
    "        start_t = time.time() # time.time() returns the current time\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(images)\n",
    "        loss = F.cross_entropy(logits, labels)  # Note that the cross_entropy function has already include the softmax function\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_this_epo=0\n",
    "        for param_group in optimizer.param_groups:\n",
    "            lr_this_epo = param_group['lr']\n",
    "        print('Train: %s, epo %s/%s, iter %s/%s, lr %.8f, %.2f img/sec, loss %.4f, time %s' \\\n",
    "            % (args.model_name, epo, args.epoch_max, it+1, len(train_loader), lr_this_epo, len(names)/(time.time()-start_t), float(loss),\n",
    "              datetime.datetime.now().replace(microsecond=0)-start_datetime))\n",
    "        if accIter['train'] % 1 == 0:\n",
    "            writer.add_scalar('Train/loss', loss, accIter['train'])\n",
    "        view_figure = True # note that I have not colorized the GT and predictions here\n",
    "        if accIter['train'] % 500 == 0:\n",
    "            if view_figure:\n",
    "                input_rgb_images = vutils.make_grid(images[:,:3], nrow=8, padding=10) # can only display 3-channel images, so images[:,:3]\n",
    "                writer.add_image('Train/input_rgb_images', input_rgb_images, accIter['train'])\n",
    "                scale = max(1, 255//args.n_class) # label (0,1,2..) is invisable, multiply a constant for visualization\n",
    "                groundtruth_tensor = labels.unsqueeze(1) * scale  # mini_batch*480*640 -> mini_batch*1*480*640\n",
    "                groundtruth_tensor = torch.cat((groundtruth_tensor, groundtruth_tensor, groundtruth_tensor), 1)  # change to 3-channel for visualization\n",
    "                groudtruth_images = vutils.make_grid(groundtruth_tensor, nrow=8, padding=10)\n",
    "                writer.add_image('Train/groudtruth_images', groudtruth_images, accIter['train'])\n",
    "                predicted_tensor = logits.argmax(1).unsqueeze(1) * scale # mini_batch*args.n_class*480*640 -> mini_batch*480*640 -> mini_batch*1*480*640\n",
    "                predicted_tensor = torch.cat((predicted_tensor, predicted_tensor, predicted_tensor),1) # change to 3-channel for visualization, mini_batch*1*480*640\n",
    "                predicted_images = vutils.make_grid(predicted_tensor, nrow=8, padding=10)\n",
    "                writer.add_image('Train/predicted_images', predicted_images, accIter['train'])\n",
    "        accIter['train'] = accIter['train'] + 1\n",
    "\n",
    "\n",
    "def validation(epo, model, val_loader): \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for it, (images, labels, names) in enumerate(val_loader):\n",
    "            images = Variable(images).to('cpu')\n",
    "            labels = Variable(labels).to('cpu')\n",
    "            start_t = time.time() # time.time() returns the current time\n",
    "            logits = model(images)\n",
    "            loss = F.cross_entropy(logits, labels)  # Note that the cross_entropy function has already include the softmax function\n",
    "            print('Val: %s, epo %s/%s, iter %s/%s, %.2f img/sec, loss %.4f, time %s' \\\n",
    "                  % (args.model_name, epo, args.epoch_max, it + 1, len(val_loader), len(names)/(time.time()-start_t), float(loss),\n",
    "                    datetime.datetime.now().replace(microsecond=0)-start_datetime))\n",
    "            if accIter['val'] % 1 == 0:\n",
    "                writer.add_scalar('Validation/loss', loss, accIter['val'])\n",
    "            view_figure = False  # note that I have not colorized the GT and predictions here\n",
    "            if accIter['val'] % 100 == 0:\n",
    "                if view_figure:\n",
    "                    input_rgb_images = vutils.make_grid(images[:, :3], nrow=8, padding=10)  # can only display 3-channel images, so images[:,:3]\n",
    "                    writer.add_image('Validation/input_rgb_images', input_rgb_images, accIter['val'])\n",
    "                    scale = max(1, 255 // args.n_class)  # label (0,1,2..) is invisable, multiply a constant for visualization\n",
    "                    groundtruth_tensor = labels.unsqueeze(1) * scale  # mini_batch*480*640 -> mini_batch*1*480*640\n",
    "                    groundtruth_tensor = torch.cat((groundtruth_tensor, groundtruth_tensor, groundtruth_tensor), 1)  # change to 3-channel for visualization\n",
    "                    groudtruth_images = vutils.make_grid(groundtruth_tensor, nrow=8, padding=10)\n",
    "                    writer.add_image('Validation/groudtruth_images', groudtruth_images, accIter['val'])\n",
    "                    predicted_tensor = logits.argmax(1).unsqueeze(1)*scale  # mini_batch*args.n_class*480*640 -> mini_batch*480*640 -> mini_batch*1*480*640\n",
    "                    predicted_tensor = torch.cat((predicted_tensor, predicted_tensor, predicted_tensor), 1)  # change to 3-channel for visualization, mini_batch*1*480*640\n",
    "                    predicted_images = vutils.make_grid(predicted_tensor, nrow=8, padding=10)\n",
    "                    writer.add_image('Validation/predicted_images', predicted_images, accIter['val'])\n",
    "            accIter['val'] += 1\n",
    "\n",
    "def testing(epo, model, test_loader):\n",
    "    model.eval()\n",
    "    conf_total = np.zeros((args.n_class, args.n_class))\n",
    "    label_list = [\"unlabeled\", \"car\", \"person\", \"bike\", \"curve\", \"car_stop\", \"guardrail\", \"color_cone\", \"bump\"]\n",
    "    testing_results_file = os.path.join(weight_dir, 'testing_results_file.txt')\n",
    "    with torch.no_grad():\n",
    "        for it, (images, labels, names) in enumerate(test_loader):\n",
    "            images = Variable(images).to('cpu')\n",
    "            labels = Variable(labels).to('cpu')\n",
    "            logits = model(images)\n",
    "            label = labels.cpu().numpy().squeeze().flatten()\n",
    "            prediction = logits.argmax(1).cpu().numpy().squeeze().flatten() # prediction and label are both 1-d array, size: minibatch*640*480\n",
    "            conf = confusion_matrix(y_true=label, y_pred=prediction, labels=[0,1,2,3,4,5,6,7,8]) # conf is args.n_class*args.n_class matrix, vertical axis: groundtruth, horizontal axis: prediction\n",
    "            conf_total += conf\n",
    "            print('Test: %s, epo %s/%s, iter %s/%s, time %s' % (args.model_name, epo, args.epoch_max, it+1, len(test_loader),\n",
    "                 datetime.datetime.now().replace(microsecond=0)-start_datetime))\n",
    "    precision, recall, IoU = compute_results(conf_total)\n",
    "    writer.add_scalar('Test/average_precision',precision.mean(), epo)\n",
    "    writer.add_scalar('Test/average_recall', recall.mean(), epo)\n",
    "    writer.add_scalar('Test/average_IoU', IoU.mean(), epo)\n",
    "    for i in range(len(precision)):\n",
    "        writer.add_scalar(\"Test(class)/precision_class_%s\" % label_list[i], precision[i], epo)\n",
    "        writer.add_scalar(\"Test(class)/recall_class_%s\"% label_list[i], recall[i],epo)\n",
    "        writer.add_scalar('Test(class)/Iou_%s'% label_list[i], IoU[i], epo)\n",
    "    if epo==0:\n",
    "        with open(testing_results_file, 'w') as f:\n",
    "            f.write(\"# %s, initial lr: %s, batch size: %s, date: %s \\n\" %(args.model_name, args.lr_start, args.batch_size, datetime.date.today()))\n",
    "            f.write(\"# epoch: unlabeled, car, person, bike, curve, car_stop, guardrail, color_cone, bump, average(nan_to_num). (Acc %, IoU %)\\n\")\n",
    "    with open(testing_results_file, 'a') as f:\n",
    "        f.write(str(epo)+': ')\n",
    "        for i in range(len(precision)):\n",
    "            f.write('%0.4f, %0.4f, ' % (100*recall[i], 100*IoU[i]))\n",
    "        f.write('%0.4f, %0.4f\\n' % (100*np.mean(np.nan_to_num(recall)), 100*np.mean(np.nan_to_num(IoU))))\n",
    "    print('saving testing results.')\n",
    "    with open(testing_results_file, \"r\") as file:\n",
    "        writer.add_text('testing_results', file.read().replace('\\n', '  \\n'), epo)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "   \n",
    "\n",
    "    args.gpu = -1  # 禁用 GPU\n",
    "\n",
    "    model = eval(args.model_name)(n_class=args.n_class)\n",
    "    model = model.to('cpu')  # 将模型移动到 CPU\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr_start, momentum=0.9, weight_decay=0.0005)\n",
    "    scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=args.lr_decay, last_epoch=-1)\n",
    "\n",
    "    # preparing folders\n",
    "    weight_dir = os.path.join(\"./runs\", args.model_name)\n",
    "    print('from epoch %d / %s' % (args.epoch_from, args.epoch_max))\n",
    "    print('weight will be saved in: %s' % weight_dir)\n",
    "\n",
    "    train_dataset = MF_dataset(data_dir=args.data_dir, split='train', transform=augmentation_methods)\n",
    "    val_dataset  = MF_dataset(data_dir=args.data_dir, split='val')\n",
    "    test_dataset = MF_dataset(data_dir=args.data_dir, split='test')\n",
    "\n",
    "    train_loader  = DataLoader(\n",
    "        dataset     = train_dataset,\n",
    "        batch_size  = args.batch_size,\n",
    "        shuffle     = True,\n",
    "        num_workers = args.num_workers,\n",
    "        pin_memory  = True,\n",
    "        drop_last   = False\n",
    "    )\n",
    "    val_loader  = DataLoader(\n",
    "        dataset     = val_dataset,\n",
    "        batch_size  = args.batch_size,\n",
    "        shuffle     = False,\n",
    "        num_workers = args.num_workers,\n",
    "        pin_memory  = True,\n",
    "        drop_last   = False\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        dataset      = test_dataset,\n",
    "        batch_size   = args.batch_size,\n",
    "        shuffle      = False,\n",
    "        num_workers  = args.num_workers,\n",
    "        pin_memory   = True,\n",
    "        drop_last    = False\n",
    "    )\n",
    "    start_datetime = datetime.datetime.now().replace(microsecond=0)\n",
    "    accIter = {'train': 0, 'val': 0}\n",
    "    for epo in range(args.epoch_from, args.epoch_max):\n",
    "        print('\\ntrain %s, epo #%s begin...' % (args.model_name, epo))\n",
    "        train(epo, model, train_loader, optimizer)\n",
    "        validation(epo, model, val_loader)\n",
    "\n",
    "    checkpoint_model_file = os.path.join(weight_dir, str(epo) + '.pth')\n",
    "    print('saving check point %s: ' % checkpoint_model_file)\n",
    "    torch.save(model.state_dict(), checkpoint_model_file)\n",
    "\n",
    "    testing(epo, model, test_loader) # testing is just for your reference, you can comment this line during training\n",
    "    scheduler.step() # if using pytorch 1.1 or above, please put this statement here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 测试运行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./weights_backup/RTFNET_152\n",
      "final.pth\n",
      "./weights_backup/RTFNET_152/final.pth\n",
      "frame 1/20, ('01500D',), time cost: 1292.17 ms, demo result saved.\n",
      "frame 2/20, ('01501D',), time cost: 1002.97 ms, demo result saved.\n",
      "frame 3/20, ('01502D',), time cost: 952.24 ms, demo result saved.\n",
      "frame 4/20, ('01503D',), time cost: 944.39 ms, demo result saved.\n",
      "frame 5/20, ('01504D',), time cost: 1034.05 ms, demo result saved.\n",
      "frame 6/20, ('01505D',), time cost: 1127.76 ms, demo result saved.\n",
      "frame 7/20, ('01506D',), time cost: 999.05 ms, demo result saved.\n",
      "frame 8/20, ('01507D',), time cost: 955.55 ms, demo result saved.\n",
      "frame 9/20, ('01508D',), time cost: 972.94 ms, demo result saved.\n",
      "frame 10/20, ('01509D',), time cost: 943.12 ms, demo result saved.\n",
      "frame 11/20, ('01200N',), time cost: 963.94 ms, demo result saved.\n",
      "frame 12/20, ('01201N',), time cost: 979.98 ms, demo result saved.\n",
      "frame 13/20, ('01202N',), time cost: 986.95 ms, demo result saved.\n",
      "frame 14/20, ('01203N',), time cost: 988.78 ms, demo result saved.\n",
      "frame 15/20, ('01204N',), time cost: 985.51 ms, demo result saved.\n",
      "frame 16/20, ('01205N',), time cost: 965.68 ms, demo result saved.\n",
      "frame 17/20, ('01206N',), time cost: 949.49 ms, demo result saved.\n",
      "frame 18/20, ('01207N',), time cost: 967.10 ms, demo result saved.\n",
      "frame 19/20, ('01208N',), time cost: 956.30 ms, demo result saved.\n",
      "frame 20/20, ('01209N',), time cost: 997.12 ms, demo result saved.\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(description='Test with pytorch')\n",
    "\n",
    "parser.add_argument('--model_name', '-m', type=str, default='RTFNet')\n",
    "parser.add_argument('--weight_name', '-w', type=str, default='RTFNET_152') # RTFNet_152, RTFNet_50, please change the number of layers in the network file\n",
    "parser.add_argument('--file_name', '-fi', type=str, default='final.pth')\n",
    "parser.add_argument('--dataset_split', '-d', type=str, default='test') # test, test_day, test_night\n",
    "parser.add_argument('--img_height', '-ih', type=int, default=480)\n",
    "parser.add_argument('--img_width', '-iw', type=int, default=640)  \n",
    "parser.add_argument('--num_workers', '-j', type=int, default=8)\n",
    "parser.add_argument('--n_class', '-nc', type=int, default=9)\n",
    "parser.add_argument('--data_dir', '-dr', type=str, default='./dataset/')\n",
    "parser.add_argument('--model_dir', '-wd', type=str, default='./weights_backup/')\n",
    "args = parser.parse_args()\n",
    "args.file_name = \"final.pth\" \n",
    "if __name__ == '__main__':\n",
    "  \n",
    "    args.gpu=-1\n",
    "    os.chmod(\"./runs\", stat.S_IRWXO)\n",
    "    model_dir = os.path.join(args.model_dir, args.weight_name)\n",
    "    # if os.path.exists(model_dir) is False:\n",
    "    #     sys.exit(\"the %s does not exit.\" %(model_dir))\n",
    "    print(model_dir)\n",
    "    print(args.file_name)\n",
    "    model_file = os.path.join(model_dir, args.file_name)\n",
    "    print(model_file)\n",
    "\n",
    "    conf_total = np.zeros((args.n_class, args.n_class))\n",
    "    model = eval(args.model_name)(n_class=args.n_class)\n",
    "    model = model.to('cpu')\n",
    "\n",
    "    pretrained_weight = torch.load(model_file, map_location='cpu', weights_only=True)\n",
    "    own_state = model.state_dict()\n",
    "    for name, param in pretrained_weight.items():\n",
    "        if name not in own_state:\n",
    "            continue\n",
    "        own_state[name].copy_(param)\n",
    "\n",
    "    batch_size = 1\n",
    "    test_dataset  = MF_dataset(data_dir=args.data_dir, split=args.dataset_split, input_h=args.img_height, input_w=args.img_width)\n",
    "    test_loader  = DataLoader(\n",
    "        dataset     = test_dataset,\n",
    "        batch_size  = batch_size,\n",
    "        shuffle     = False,\n",
    "        num_workers = args.num_workers,\n",
    "        pin_memory  = True,\n",
    "        drop_last   = False\n",
    "    )\n",
    "    ave_time_cost = 0.0\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for it, (images, labels, names) in enumerate(test_loader):\n",
    "            images = Variable(images).to('cpu')\n",
    "            labels = Variable(labels).to('cpu')\n",
    "            start_time = time.time()\n",
    "            logits = model(images)  # logits.size(): mini_batch*num_class*480*640\n",
    "            end_time = time.time()\n",
    "            if it>=5: # # ignore the first 5 frames\n",
    "                ave_time_cost += (end_time-start_time)\n",
    "            # convert tensor to numpy 1d array\n",
    "            label = labels.cpu().numpy().squeeze().flatten()\n",
    "            prediction = logits.argmax(1).cpu().numpy().squeeze().flatten() # prediction and label are both 1-d array, size: minibatch*640*480\n",
    "            # generate confusion matrix frame-by-frame\n",
    "            # conf = confusion_matrix(y_true=label, y_pred=prediction, labels=[0,1,2,3,4,5,6,7,8]) # conf is an n_class*n_class matrix, vertical axis: groundtruth, horizontal axis: prediction\n",
    "            # conf_total += conf\n",
    "            # save demo images\n",
    "            visualize(image_name=names, predictions=logits.argmax(1), weight_name=args.weight_name)\n",
    "            print(\"frame %d/%d, %s, time cost: %.2f ms, demo result saved.\"\n",
    "                  %(it+1, len(test_loader), names, (end_time-start_time)*1000))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1600x800 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "pic = \"1500D\"\n",
    "# 指定图像路径（例如测试集中的第一张图像）\n",
    "image_path = 'runs/Predict_RTFNET_152_0{}.png'.format(pic)  # 根据实际路径调整\n",
    "image_path1 = 'runs/Pred_RTFNet_152_0{}.png'.format(pic)\n",
    "plt.figure(figsize=(16, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "img = plt.imread(image_path)\n",
    "plt.imshow(img)\n",
    "plt.axis('off')  # 隐藏坐标轴\n",
    "plt.title(\"Modified\")\n",
    "plt.subplot(1, 2, 2)\n",
    "img1 = plt.imread(image_path1)\n",
    "plt.imshow(img1)\n",
    "plt.axis('off')  # 隐藏坐标轴\n",
    "plt.title(\"Original\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multimodel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
